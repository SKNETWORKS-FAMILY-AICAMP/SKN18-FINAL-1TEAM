import asyncio
import numpy as np
import urllib.parse
from playwright.async_api import async_playwright
import json
import os
import sys
from global_land_mask import globe
import time

# ======================================================
# 1. [ì„¤ì •] ì„œìš¸ ì „ì—­ ì¢Œí‘œ ë° 5ê°œ ê·¸ë£¹ ì •ì˜
# ======================================================
ZOOM_LEVEL = 16
LAT_STEP = 0.015
LNG_STEP = 0.015

TARGET_ZONES = {
    # --- 1ê·¸ë£¹: ê°•ë‚¨ê¶Œ ---
    "ê°•ë‚¨êµ¬": {"lat_min": 37.470, "lat_max": 37.535, "lng_min": 127.010, "lng_max": 127.080},
    "ì„œì´ˆêµ¬": {"lat_min": 37.440, "lat_max": 37.520, "lng_min": 126.990, "lng_max": 127.050},
    "ì†¡íŒŒêµ¬": {"lat_min": 37.470, "lat_max": 37.530, "lng_min": 127.070, "lng_max": 127.150},
    "ê°•ë™êµ¬": {"lat_min": 37.520, "lat_max": 37.580, "lng_min": 127.110, "lng_max": 127.190},

    # --- 2ê·¸ë£¹: ì„œë‚¨ê¶Œ ---
    "ê´€ì•…êµ¬": {"lat_min": 37.455, "lat_max": 37.490, "lng_min": 126.900, "lng_max": 126.970},
    "ë™ì‘êµ¬": {"lat_min": 37.475, "lat_max": 37.515, "lng_min": 126.905, "lng_max": 126.985},
    "ì˜ë“±í¬êµ¬": {"lat_min": 37.490, "lat_max": 37.555, "lng_min": 126.880, "lng_max": 126.950},
    "êµ¬ë¡œêµ¬": {"lat_min": 37.460, "lat_max": 37.510, "lng_min": 126.810, "lng_max": 126.900},
    "ê¸ˆì²œêµ¬": {"lat_min": 37.430, "lat_max": 37.485, "lng_min": 126.870, "lng_max": 126.915},

    # --- 3ê·¸ë£¹: ì„œë¶ê¶Œ ---
    "ê°•ì„œêµ¬": {"lat_min": 37.530, "lat_max": 37.570, "lng_min": 126.820, "lng_max": 126.880},
    "ì–‘ì²œêµ¬": {"lat_min": 37.505, "lat_max": 37.555, "lng_min": 126.820, "lng_max": 126.890},
    "ë§ˆí¬êµ¬": {"lat_min": 37.535, "lat_max": 37.575, "lng_min": 126.880, "lng_max": 126.960},
    "ì„œëŒ€ë¬¸êµ¬": {"lat_min": 37.550, "lat_max": 37.600, "lng_min": 126.900, "lng_max": 126.970},
    "ì€í‰êµ¬": {"lat_min": 37.570, "lat_max": 37.650, "lng_min": 126.880, "lng_max": 126.950},

    # --- 4ê·¸ë£¹: ë„ì‹¬ & ì„±ë™ê´‘ì§„ ---
    "ì¢…ë¡œêµ¬": {"lat_min": 37.560, "lat_max": 37.630, "lng_min": 126.950, "lng_max": 127.020},
    "ì¤‘êµ¬":   {"lat_min": 37.540, "lat_max": 37.570, "lng_min": 126.960, "lng_max": 127.020},
    "ìš©ì‚°êµ¬": {"lat_min": 37.510, "lat_max": 37.555, "lng_min": 126.940, "lng_max": 127.015},
    "ì„±ë™êµ¬": {"lat_min": 37.530, "lat_max": 37.575, "lng_min": 127.010, "lng_max": 127.070},
    "ê´‘ì§„êµ¬": {"lat_min": 37.525, "lat_max": 37.575, "lng_min": 127.050, "lng_max": 127.120},

    # --- 5ê·¸ë£¹: ë™ë¶ê¶Œ ---
    "ë™ëŒ€ë¬¸êµ¬": {"lat_min": 37.560, "lat_max": 37.610, "lng_min": 127.020, "lng_max": 127.080},
    "ì¤‘ë‘êµ¬": {"lat_min": 37.570, "lat_max": 37.630, "lng_min": 127.070, "lng_max": 127.120},
    "ì„±ë¶êµ¬": {"lat_min": 37.575, "lat_max": 37.620, "lng_min": 127.000, "lng_max": 127.070},
    "ê°•ë¶êµ¬": {"lat_min": 37.610, "lat_max": 37.690, "lng_min": 127.000, "lng_max": 127.050},
    "ë„ë´‰êµ¬": {"lat_min": 37.640, "lat_max": 37.690, "lng_min": 127.010, "lng_max": 127.060},
    "ë…¸ì›êµ¬": {"lat_min": 37.615, "lat_max": 37.670, "lng_min": 127.040, "lng_max": 127.100},
}

# ======================================================
# 2. ì¤‘ë³µ ID ê´€ë¦¬
# ======================================================
MASTER_ID_FILE = 'crawled_ids.txt'

def load_master_ids():
    if not os.path.exists(MASTER_ID_FILE): return set()
    try:
        with open(MASTER_ID_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except: return set()

def update_and_save_master_ids(new_ids_set):
    if not new_ids_set: return
    try:
        current_file_ids = set()
        if os.path.exists(MASTER_ID_FILE):
            with open(MASTER_ID_FILE, 'r', encoding='utf-8') as f:
                current_file_ids = {line.strip() for line in f if line.strip()}
        merged_ids = current_file_ids.union(new_ids_set)
        with open(MASTER_ID_FILE, 'w', encoding='utf-8') as f:
            for item_id in sorted(list(merged_ids)): f.write(f"{item_id}\n")
        print(f"--- [ID ì €ì¥] í†µí•© {len(merged_ids)}ê°œ ì €ì¥ ì™„ë£Œ (ì´ë²ˆ ì¶”ê°€: {len(new_ids_set)}ê°œ) ---")
    except Exception as e: print(f"--- [ID ì €ì¥ ì˜¤ë¥˜] {e}")

# ======================================================
# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ======================================================
def generate_coordinate_grid(zone_name):
    if zone_name not in TARGET_ZONES:
        print(f"Error: '{zone_name}' êµ¬ì—­ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    zone = TARGET_ZONES[zone_name]
    coordinates = []
    print(f"--- [ì¢Œí‘œ ìƒì„±] '{zone_name}' (Zoom {ZOOM_LEVEL}) ---")
    
    lat_points = np.arange(zone["lat_min"], zone["lat_max"], LAT_STEP)
    lng_points = np.arange(zone["lng_min"], zone["lng_max"], LNG_STEP)
    
    for lat in lat_points:
        for lng in lng_points:
            if globe.is_land(lat, lng):
                coordinates.append({'lat': lat, 'lng': lng, 'zoom': ZOOM_LEVEL})
    
    print(f"--- [ì™„ë£Œ] '{zone_name}' ì´ {len(coordinates)}ê°œì˜ ê²©ì ì¢Œí‘œ ìƒì„± ---")
    return coordinates

async def scroll_to_bottom(page, item_selector):
    scroll_container = ".list__wrapper"
    try:
        await page.wait_for_selector(scroll_container, timeout=5000)
        list_cont = page.locator(scroll_container)
        last_cnt = 0
        while True:
            try:
                await list_cont.click(timeout=500); await list_cont.press('PageDown'); await page.wait_for_timeout(300)
            except: pass
            
            cur_top = await list_cont.evaluate("node => node.scrollTop")
            for _ in range(5):
                try: await list_cont.press('PageDown'); await page.wait_for_timeout(200)
                except: break
                
            cur_cnt = await page.locator(item_selector).count()
            if cur_cnt == last_cnt:
                try: await page.wait_for_function(f"document.querySelectorAll('{item_selector}').length > {last_cnt}", timeout=3000); continue
                except: break
            last_cnt = cur_cnt
    except: pass

# ======================================================
# ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë©
# ======================================================
async def scrape_detail_page(new_page):
    page_url = new_page.url
    listing_id = page_url.split('/house/')[-1].split('?')[0] if '/house/' in page_url else None
    
    final_data = {
        'ì¤‘ê°œì‚¬_ì •ë³´': {},         
        'ë§¤ë¬¼ë²ˆí˜¸': listing_id,    
        'ë§¤ë¬¼_URL': page_url,      
        'ë§¤ë¬¼_ì´ë¯¸ì§€': [],         # <--- [ì‹ ê·œ] ë§¤ë¬¼ URL ë°”ë¡œ ì•„ë˜ì— ì¶”ê°€ë¨
        'ì£¼ì†Œ_ì •ë³´': {},           
        'í‰ë©´ë„_URL': [],          
        'ê±°ë˜_ì •ë³´': {},           
        'ë§¤ë¬¼_ì •ë³´': {},           
        'ì¶”ê°€_ì˜µì…˜': [],           
        'ì£¼ë³€_í•™êµ': [],           
        'ìƒì„¸_ì„¤ëª…': ""            
    }
    
    # ======================================================
    # â–¼â–¼â–¼ [ì‹ ê·œ] 1. ë§¤ë¬¼ ì´ë¯¸ì§€ (Carousel) ì „ì²´ ìˆ˜ì§‘ â–¼â–¼â–¼
    # ======================================================
    try:
        # ì‚¬ì§„ ì† êµ¬ì¡°: .carousel-inner > .item > img.photo
        # ëª¨ë“  ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        img_elements = await new_page.locator(".carousel-inner .item img.photo").all()
        
        for img in img_elements:
            src = await img.get_attribute("src")
            if src and src not in final_data['ë§¤ë¬¼_ì´ë¯¸ì§€']:
                final_data['ë§¤ë¬¼_ì´ë¯¸ì§€'].append(src)
                
        # (í˜¹ì‹œ ë¡œë”©ì´ ì•ˆ ëì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ container ëŒ€ê¸° í›„ ì¬ì‹œë„)
        if not final_data['ë§¤ë¬¼_ì´ë¯¸ì§€']:
            await new_page.wait_for_selector(".carousel-inner", timeout=2000)
            img_elements = await new_page.locator(".carousel-inner img").all()
            for img in img_elements:
                src = await img.get_attribute("src")
                if src and src not in final_data['ë§¤ë¬¼_ì´ë¯¸ì§€']:
                    final_data['ë§¤ë¬¼_ì´ë¯¸ì§€'].append(src)
                    
    except Exception as e:
        # ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨í•´ë„ ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ë„˜ì–´ê°
        # print(f"  -> (ì´ë¯¸ì§€ ì¶”ì¶œ ì˜¤ë¥˜) {e}") 
        pass
    # ======================================================

    # 2. ì£¼ì†Œ
    try:
        addr_sel = ".div-detail-house-address-type > span, span.address"
        await new_page.wait_for_selector(addr_sel, timeout=3000)
        final_data['ì£¼ì†Œ_ì •ë³´']['ì „ì²´ì£¼ì†Œ'] = (await new_page.locator(addr_sel).first.inner_text()).strip()
    except: pass

    # 3. ì¤‘ê°œì‚¬ (ë‚´ë¶€ í‚¤ê°’ë„ í•œê¸€ë¡œ ë³€ê²½)
    try:
        sb = new_page.locator(".sidebar-content").first
        if await sb.count() > 0:
            agency_name_selector = ".agency-name"
            if await sb.locator(agency_name_selector).count() > 0:
                final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ì¤‘ê°œì‚¬ëª…'] = (await sb.locator(agency_name_selector).inner_text()).strip()
            
            txt = await sb.inner_text()
            for line in txt.split('\n'):
                line = line.strip()
                if not line: continue
                if "ëŒ€í‘œë²ˆí˜¸" in line: final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ì „í™”ë²ˆí˜¸'] = line.replace("ëŒ€í‘œë²ˆí˜¸", "").strip()
                elif "ì£¼ì†Œ" in line: final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ì£¼ì†Œ'] = line.replace("ì£¼ì†Œ", "").strip()
                elif "ë“±ë¡ë²ˆí˜¸" in line: final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ë“±ë¡ë²ˆí˜¸'] = line.replace("ì¤‘ê°œì‚¬ë¬´ì†Œ", "").replace("ë“±ë¡ë²ˆí˜¸", "").strip()
                elif "ëŒ€í‘œì" in line: final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ëŒ€í‘œì'] = line.replace("ëŒ€í‘œì", "").strip()
                elif "ëŒ€í‘œ " in line and "ëŒ€í‘œì" not in final_data['ì¤‘ê°œì‚¬_ì •ë³´']: final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ë‹´ë‹¹ì'] = line.replace("ëŒ€í‘œ", "").strip()

            # ê±°ë˜ì™„ë£Œ/ë“±ë¡ë§¤ë¬¼
            stat_items = await sb.locator(".agency-house .item-wrapper").all()
            for item in stat_items:
                try:
                    title_el = item.locator(".agency-house-title")
                    count_el = item.locator(".agency-house-count")
                    if await title_el.count() > 0 and await count_el.count() > 0:
                        title = (await title_el.inner_text()).strip()
                        count = (await count_el.inner_text()).strip()
                        if "ê±°ë˜ì™„ë£Œ" in title: final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ê±°ë˜ì™„ë£Œ'] = count
                        elif "ë“±ë¡ë§¤ë¬¼" in title: final_data['ì¤‘ê°œì‚¬_ì •ë³´']['ë“±ë¡ë§¤ë¬¼'] = count
                except: pass
    except: pass

    # 4. í‰ë©´ë„
    try:
        plan_imgs = await new_page.locator("div[id^='aptPlanImage'] img").all()
        for img in plan_imgs:
            src = await img.get_attribute("src")
            if src and src not in final_data['í‰ë©´ë„_URL']:
                final_data['í‰ë©´ë„_URL'].append(src)
        if not final_data['í‰ë©´ë„_URL']:
             fallback_imgs = await new_page.locator(".detail-aptPlanImage img").all()
             for img in fallback_imgs:
                src = await img.get_attribute("src")
                if src and src not in final_data['í‰ë©´ë„_URL']:
                    final_data['í‰ë©´ë„_URL'].append(src)
    except: pass

    # 5. ìƒì„¸ ì •ë³´
    rows = await new_page.locator(".detail-table-row").all()
    for r in rows:
        try:
            k = (await r.locator(".detail-table-th").inner_text()).strip()
            v = (await r.locator(".detail-table-td").inner_text()).strip().replace('\n', ' ')
            if k in ["ê±°ë˜ë°©ì‹", "ê´€ë¦¬ë¹„", "ìœµìê¸ˆ", "ì…ì£¼ê°€ëŠ¥ì¼"]: final_data['ê±°ë˜_ì •ë³´'][k] = v
            else: final_data['ë§¤ë¬¼_ì •ë³´'][k] = v
        except: pass

    # 6. ì¶”ê°€ì˜µì…˜
    try:
        options_container = new_page.locator(".detail-option-table dd")
        if await options_container.count() > 0:
            all_options = await options_container.all()
            for opt in all_options:
                text = await opt.inner_text()
                if text.strip():
                    final_data['ì¶”ê°€_ì˜µì…˜'].append(text.strip())
    except: pass

    # 7. ì£¼ë³€í•™êµ
    try:
        school_header = new_page.locator("h3", has_text="ì£¼ë³€í•™êµ")
        if await school_header.count() > 0:
            school_section = school_header.locator("xpath=..")
            school_buttons = await school_section.locator("button").all()
            for btn in school_buttons:
                school_name = (await btn.inner_text()).strip()
                if not school_name: continue
                try:
                    await btn.click(); await new_page.wait_for_timeout(100)
                    addr_row = school_section.locator(".detail-table-row", has_text="ì£¼ì†Œ")
                    if await addr_row.count() > 0:
                        full_text = await addr_row.inner_text()
                        address = full_text.replace("ì£¼ì†Œ", "").strip()
                        final_data['ì£¼ë³€_í•™êµ'].append({"í•™êµëª…": school_name, "ì£¼ì†Œ": address})
                except: pass
    except: pass

    # 8. ìƒì„¸ì„¤ëª…
    try:
        desc_selector = "#description-text"
        if await new_page.locator(desc_selector).count() > 0:
            desc_text = await new_page.locator(desc_selector).inner_text()
            final_data['ìƒì„¸_ì„¤ëª…'] = desc_text.strip()
    except: pass
    
    print("  ->  ... (ìƒˆ íƒ­) ìƒì„¸ ì •ë³´ ìŠ¤í¬ë© ì™„ë£Œ.")
    return final_data

async def extract_data_from_list(page, item_selector, all_data, master_set, current_session_ids):
    items = await page.locator(item_selector).all()
    for item in items:
        try:
            hid = await item.get_attribute('data-hidx')
            if hid and hid in master_set: continue 
            
            async with page.context.expect_page() as event:
                await item.click(timeout=3000)
            np = await event.value
            await np.wait_for_load_state('domcontentloaded')
            
            data = await scrape_detail_page(np)
            
            if not data['ë§¤ë¬¼ë²ˆí˜¸'] and hid: data['ë§¤ë¬¼ë²ˆí˜¸'] = hid
            
            if data['ë§¤ë¬¼ë²ˆí˜¸']:
                all_data.append(data)
                master_set.add(data['ë§¤ë¬¼ë²ˆí˜¸']) 
                current_session_ids.add(data['ë§¤ë¬¼ë²ˆí˜¸']) 
            
            await np.close()
            await page.wait_for_timeout(500)
        except: pass

async def setup_filters(page):
    try:
        btn = page.locator("button", has_text="ì „ìš©ë©´ì ")
        if await btn.count() == 0: btn = page.locator("button", has_text="í‰")
        if await btn.count() > 0:
            await btn.first.click(); await page.wait_for_timeout(1000)
            handles = page.locator(".vue-slider-dot-handle")
            if await handles.count() >= 2:
                box = await handles.nth(1).bounding_box()
                if box:
                    await page.mouse.move(box['x']+box['width']/2, box['y']+box['height']/2)
                    await page.mouse.down()
                    await page.mouse.move(box['x']-230, box['y'], steps=10)
                    await page.mouse.up()
                    await page.wait_for_timeout(1000)
    except: pass

# ======================================================
# 4. í•µì‹¬ ì‹¤í–‰ ë¡œì§
# ======================================================
async def run_zone_batch(zone_name):
    master_id_set = load_master_ids()
    current_session_new_ids = set()

    AREA_FILTER_PREFIX = "checkRealSize:999~40||"
    CATEGORIES = [
        {"name": "ì•„íŒŒíŠ¸", "base": "https://www.peterpanz.com/apt", "filt": AREA_FILTER_PREFIX + 'buildingType;["ì•„íŒŒíŠ¸"]', "out": f"{zone_name}_ì•„íŒŒíŠ¸.json"},
        {"name": "ì›,íˆ¬ë£¸", "base": "https://www.peterpanz.com/onetworoom", "filt": AREA_FILTER_PREFIX + 'buildingType;["ì›,íˆ¬ë£¸"]', "out": f"{zone_name}_ì›íˆ¬ë£¸.json"},
        {"name": "ë¹Œë¼,ì£¼íƒ", "base": "https://www.peterpanz.com/villa", "filt": AREA_FILTER_PREFIX + 'buildingType;["ë¹Œë¼","ì£¼íƒ"]', "out": f"{zone_name}_ë¹Œë¼ì£¼íƒ.json"},
        {"name": "ì˜¤í”¼ìŠ¤í…”", "base": "https://www.peterpanz.com/officetel", "filt": AREA_FILTER_PREFIX + 'buildingType;["ì˜¤í”¼ìŠ¤í…”"]', "out": f"{zone_name}_ì˜¤í”¼ìŠ¤í…”.json"},
        # {"name": "ìƒê°€âˆ™ì‚¬ë¬´ì‹¤âˆ™ê±´ë¬¼âˆ™ê³µì¥âˆ™í† ì§€", "base": "https://www.peterpanz.com/store", "filt": AREA_FILTER_PREFIX + 'buildingType;["ìƒê°€","ì‚¬ë¬´ì‹¤","ê±´ë¬¼","ê³µì¥","í† ì§€"]', "out": f"{zone_name}_ìƒê°€.json"}
    ]

    coordinates = generate_coordinate_grid(zone_name)
    if not coordinates: return

    ITEM_SELECTOR = ".a-house"
    current_processing_data = []
    current_processing_filename = ""

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            page = await browser.new_page()
            
            for cat in CATEGORIES:
                print(f"\nğŸš€ğŸš€ğŸš€ [{zone_name}] ì¹´í…Œê³ ë¦¬: '{cat['name']}' ì‹œì‘ ğŸš€ğŸš€ğŸš€")
                current_processing_data = []
                current_processing_filename = cat['out']
                
                try:
                    await page.goto(cat['base'], timeout=60000)
                    try:
                        popup = page.get_by_role("button", name="ì˜¤ëŠ˜ í•˜ë£¨ ë³´ì§€ ì•Šê¸°")
                        if await popup.is_visible(): await popup.click()
                    except: pass
                    await page.wait_for_timeout(2000)
                    await setup_filters(page)

                    for i, coord in enumerate(coordinates):
                        lat, lng, zoom = coord['lat'], coord['lng'], coord['zoom']
                        center = urllib.parse.quote(f'{{"y":{lat},"_lat":{lat},"x":{lng},"_lng":{lng}}}')
                        filt = urllib.parse.quote(cat['filt'], safe=':;~')
                        target_url = f"{cat['base']}?zoomLevel={zoom}&center={center}&filter={filt}"
                        
                        print(f"({i+1}/{len(coordinates)}) ì´ë™...")
                        try:
                            await page.goto(target_url, timeout=30000)
                            await page.wait_for_load_state('networkidle'); await page.wait_for_timeout(500)
                            if await page.locator(ITEM_SELECTOR).count() == 0: continue
                            
                            await scroll_to_bottom(page, ITEM_SELECTOR)
                            await extract_data_from_list(page, ITEM_SELECTOR, current_processing_data, master_id_set, current_session_new_ids)
                        except: pass

                except Exception as e:
                    print(f"Error in category loop: {e}")
                
                if current_processing_data:
                    with open(current_processing_filename, 'w', encoding='utf-8') as f:
                        json.dump(current_processing_data, f, ensure_ascii=False, indent=2)
                    print(f"âœ… '{current_processing_filename}' ì €ì¥ ì™„ë£Œ.")
                    update_and_save_master_ids(current_session_new_ids)
                    current_session_new_ids.clear()

            await browser.close()
            
    except (KeyboardInterrupt, asyncio.CancelledError):
        print(f"\n\nğŸš¨ --- [{zone_name}] ì‚¬ìš©ì ì¤‘ë‹¨ ë°œìƒ! í˜„ì¬ ë°ì´í„° ì €ì¥ ì‹œë„... ---")
        if current_processing_data and current_processing_filename:
            try:
                with open(current_processing_filename, 'w', encoding='utf-8') as f:
                    json.dump(current_processing_data, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ [ë¹„ìƒ ì €ì¥] '{current_processing_filename}'ì— {len(current_processing_data)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ.")
            except Exception as e:
                print(f"âŒ [ë¹„ìƒ ì €ì¥ ì‹¤íŒ¨] {e}")

    finally:
        update_and_save_master_ids(current_session_new_ids)
        print(f"--- [{zone_name}] ì‘ì—… ì¢…ë£Œ ---")

# ======================================================
# 5. ì‹¤í–‰ë¶€
# ======================================================
if __name__ == "__main__":

    # --- [ì‹ ê·œ] ì‹œê°„ ì¸¡ì • ì‹œì‘ ---
    TOTAL_START_TIME = time.time()

    if len(sys.argv) > 1:
        TERMINAL_NUMBER = int(sys.argv[1])
    else:
        print("âš ï¸ í„°ë¯¸ë„ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ 1ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        TERMINAL_NUMBER = 1

    GROUPS = {
        1: ["ê°•ë‚¨êµ¬", "ì„œì´ˆêµ¬", "ì†¡íŒŒêµ¬", "ê°•ë™êµ¬"],
        2: ["ê´€ì•…êµ¬", "ë™ì‘êµ¬", "ì˜ë“±í¬êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬"],
        3: ["ê°•ì„œêµ¬", "ì–‘ì²œêµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì€í‰êµ¬"],
        4: ["ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ìš©ì‚°êµ¬", "ì„±ë™êµ¬", "ê´‘ì§„êµ¬"],
        5: ["ë™ëŒ€ë¬¸êµ¬", "ì¤‘ë‘êµ¬", "ì„±ë¶êµ¬", "ê°•ë¶êµ¬", "ë„ë´‰êµ¬", "ë…¸ì›êµ¬"]
    }

    target_zones_list = GROUPS.get(TERMINAL_NUMBER, [])

    if not target_zones_list:
        print(f"âŒ ì˜ëª»ëœ í„°ë¯¸ë„ ë²ˆí˜¸ì…ë‹ˆë‹¤: {TERMINAL_NUMBER} (1~5 ì‚¬ì´ ì…ë ¥)")
    else:
        print(f"========== [í„°ë¯¸ë„ {TERMINAL_NUMBER}] ë³‘ë ¬ ì‘ì—… ì‹œì‘ ==========")
        print(f"ëŒ€ìƒ êµ¬ì—­: {target_zones_list}")
        
        for zone in target_zones_list:
            asyncio.run(run_zone_batch(zone))

        # --- [ì‹ ê·œ] ì‹œê°„ ì¸¡ì • ì¢…ë£Œ ë° ì¶œë ¥ ---
        TOTAL_END_TIME = time.time()
        elapsed = TOTAL_END_TIME - TOTAL_START_TIME
        hours, rem = divmod(elapsed, 3600)
        minutes, seconds = divmod(rem, 60)
            
        print(f"========== [í„°ë¯¸ë„ {TERMINAL_NUMBER}] ëª¨ë“  ì‘ì—… ì¢…ë£Œ ==========")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {int(hours)}ì‹œê°„ {int(minutes)}ë¶„ {int(seconds)}ì´ˆ")